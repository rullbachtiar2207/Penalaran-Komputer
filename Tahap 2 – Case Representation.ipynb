{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BJaghH9U2RN",
        "outputId": "eb513d7b-0aca-4b01-d15b-f582773bc7b5"
      },
      "id": "6BJaghH9U2RN",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install pdfplumber pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M611ROwiVJZZ",
        "outputId": "06e212ff-e1d6-4f5f-c82d-5e217c1af694"
      },
      "id": "M611ROwiVJZZ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Case Representation System with minimal dependencies\n",
        "# First, install required packages\n",
        "# pip install pdfplumber pandas\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional\n",
        "import pdfplumber\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class SimpleCaseRepresentationSystem:\n",
        "    def __init__(self, pdf_path: str, csv_path: str):\n",
        "        \"\"\"\n",
        "        Initialize Simple Case Representation System\n",
        "\n",
        "        Args:\n",
        "            pdf_path: Path to PDF files directory\n",
        "            csv_path: Path to save processed CSV files\n",
        "        \"\"\"\n",
        "        self.pdf_path = pdf_path\n",
        "        self.csv_path = csv_path\n",
        "        self.cases_data = []\n",
        "\n",
        "        # Indonesian stopwords (manual list)\n",
        "        self.stopwords_id = {\n",
        "            'dan', 'atau', 'yang', 'di', 'ke', 'dari', 'pada', 'dalam', 'untuk',\n",
        "            'dengan', 'oleh', 'bahwa', 'adalah', 'akan', 'telah', 'dapat', 'bisa',\n",
        "            'ini', 'itu', 'sini', 'situ', 'sana', 'mereka', 'kami', 'kita', 'saya',\n",
        "            'anda', 'dia', 'ia', 'nya', 'mu', 'ku', 'se', 'an', 'ter', 'ber', 'per',\n",
        "            'a', 'the', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'\n",
        "        }\n",
        "\n",
        "        # Create output directory if not exists\n",
        "        os.makedirs(csv_path, exist_ok=True)\n",
        "        print(f\"Initialized Case Representation System\")\n",
        "        print(f\"PDF Path: {pdf_path}\")\n",
        "        print(f\"CSV Path: {csv_path}\")\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extract text from PDF file using pdfplumber\n",
        "\n",
        "        Args:\n",
        "            pdf_file_path: Path to PDF file\n",
        "\n",
        "        Returns:\n",
        "            Extracted text content\n",
        "        \"\"\"\n",
        "        try:\n",
        "            text = \"\"\n",
        "            with pdfplumber.open(pdf_file_path) as pdf:\n",
        "                for page_num, page in enumerate(pdf.pages):\n",
        "                    try:\n",
        "                        page_text = page.extract_text()\n",
        "                        if page_text:\n",
        "                            text += page_text + \"\\n\"\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Error extracting page {page_num+1} from {pdf_file_path}: {str(e)}\")\n",
        "                        continue\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from {pdf_file_path}: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean and normalize text\n",
        "\n",
        "        Args:\n",
        "            text: Raw text\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "        return text\n",
        "\n",
        "    def extract_metadata(self, text: str, filename: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract metadata from court decision text\n",
        "\n",
        "        Args:\n",
        "            text: Full text of court decision\n",
        "            filename: PDF filename\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing extracted metadata\n",
        "        \"\"\"\n",
        "        metadata = {\n",
        "            'case_id': filename.replace('.pdf', ''),\n",
        "            'no_perkara': '',\n",
        "            'tanggal': '',\n",
        "            'jenis_perkara': '',\n",
        "            'pasal': '',\n",
        "            'pihak': '',\n",
        "            'penggugat': '',\n",
        "            'tergugat': '',\n",
        "            'filename': filename\n",
        "        }\n",
        "\n",
        "        # Clean text for better pattern matching\n",
        "        clean_text = self.clean_text(text)\n",
        "\n",
        "        # Extract case number (Nomor Perkara) - Multiple patterns\n",
        "        no_perkara_patterns = [\n",
        "            r'(?:Nomor|No\\.?|Perkara\\s+Nomor)\\s*:?\\s*(\\d+\\/[A-Za-z\\.\\s]+\\/\\d{4})',\n",
        "            r'(\\d+\\/[A-Za-z]+[\\/\\.][A-Za-z]*\\/\\d{4})',\n",
        "            r'Nomor\\s*:\\s*([^\\n]+)',\n",
        "        ]\n",
        "\n",
        "        for pattern in no_perkara_patterns:\n",
        "            match = re.search(pattern, clean_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                metadata['no_perkara'] = match.group(1).strip()\n",
        "                break\n",
        "\n",
        "        # Extract date - Multiple patterns\n",
        "        date_patterns = [\n",
        "            r'(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',  # 15 Januari 2023\n",
        "            r'(\\d{1,2})[-/](\\d{1,2})[-/](\\d{4})',  # 15/01/2023\n",
        "            r'(\\d{4})[-/](\\d{1,2})[-/](\\d{1,2})',  # 2023/01/15\n",
        "            r'tanggal\\s+(\\d{1,2})\\s+(\\w+)\\s+(\\d{4})',  # tanggal 15 Januari 2023\n",
        "        ]\n",
        "\n",
        "        for pattern in date_patterns:\n",
        "            match = re.search(pattern, clean_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                metadata['tanggal'] = match.group(0)\n",
        "                break\n",
        "\n",
        "        # Extract case type\n",
        "        case_type_patterns = [\n",
        "            r'(Perkara\\s+Perdata)',\n",
        "            r'(Perkara\\s+Pidana)',\n",
        "            r'(Pdt\\.G[^/]*)',\n",
        "            r'(Pid[^/]*)',\n",
        "            r'(Perdata)',\n",
        "            r'(Pidana)',\n",
        "            r'(Tata\\s+Usaha\\s+Negara|TUN)',\n",
        "            r'(Agama)',\n",
        "        ]\n",
        "\n",
        "        for pattern in case_type_patterns:\n",
        "            match = re.search(pattern, clean_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                metadata['jenis_perkara'] = match.group(1)\n",
        "                break\n",
        "\n",
        "        # Extract articles (Pasal)\n",
        "        pasal_patterns = [\n",
        "            r'Pasal\\s+(\\d+(?:\\s+[a-zA-Z]+)*(?:\\s+dan\\s+\\d+)*)',\n",
        "            r'ps\\.?\\s+(\\d+)',\n",
        "            r'pasal\\s+(\\d+[^\\n,\\.]{0,50})'\n",
        "        ]\n",
        "\n",
        "        pasal_matches = []\n",
        "        for pattern in pasal_patterns:\n",
        "            matches = re.findall(pattern, clean_text, re.IGNORECASE)\n",
        "            pasal_matches.extend(matches)\n",
        "\n",
        "        if pasal_matches:\n",
        "            # Remove duplicates and limit to first 5\n",
        "            unique_pasal = list(set(pasal_matches[:5]))\n",
        "            metadata['pasal'] = ', '.join(unique_pasal)\n",
        "\n",
        "        # Extract parties\n",
        "        pihak_patterns = [\n",
        "            r'(Penggugat|Pemohon|Terdakwa)\\s*:?\\s*([^\\n;]+?)(?=\\n|;|Melawan|vs|Tergugat|Termohon)',\n",
        "            r'(Tergugat|Termohon|Jaksa)\\s*:?\\s*([^\\n;]+?)(?=\\n|;|$)',\n",
        "        ]\n",
        "\n",
        "        parties = []\n",
        "        for pattern in pihak_patterns:\n",
        "            matches = re.findall(pattern, clean_text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                role, name = match\n",
        "                clean_name = name.strip()\n",
        "                if len(clean_name) > 3:  # Minimum name length\n",
        "                    parties.append(f\"{role}: {clean_name}\")\n",
        "\n",
        "                    if role.lower() in ['penggugat', 'pemohon', 'terdakwa']:\n",
        "                        metadata['penggugat'] = clean_name\n",
        "                    elif role.lower() in ['tergugat', 'termohon']:\n",
        "                        metadata['tergugat'] = clean_name\n",
        "\n",
        "        metadata['pihak'] = ' | '.join(parties[:4])  # Limit to 4 parties\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def extract_key_content(self, text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Extract key content from court decision\n",
        "\n",
        "        Args:\n",
        "            text: Full text of court decision\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing key content\n",
        "        \"\"\"\n",
        "        content = {\n",
        "            'ringkasan_fakta': '',\n",
        "            'argumen_hukum': '',\n",
        "            'putusan': '',\n",
        "            'barang_bukti': ''\n",
        "        }\n",
        "\n",
        "        clean_text = self.clean_text(text)\n",
        "\n",
        "        # Extract facts summary\n",
        "        fakta_patterns = [\n",
        "            r'(?:DUDUK\\s+PERKARA|FAKTA[^:]*):?\\s*([^A-Z\\n]{100,800})',\n",
        "            r'(?:Bahwa\\s+pada\\s+|Menimbang\\s+bahwa)[^:]*([^A-Z\\n]{100,600})',\n",
        "            r'(?:KRONOLOGI|PERISTIWA)[^:]*:?\\s*([^A-Z\\n]{100,600})'\n",
        "        ]\n",
        "\n",
        "        for pattern in fakta_patterns:\n",
        "            match = re.search(pattern, clean_text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                facts = match.group(1).strip()\n",
        "                # Clean up facts\n",
        "                facts = re.sub(r'\\s+', ' ', facts)\n",
        "                content['ringkasan_fakta'] = facts[:500]  # Limit length\n",
        "                break\n",
        "\n",
        "        # Extract legal arguments\n",
        "        argumen_patterns = [\n",
        "            r'(?:PERTIMBANGAN\\s+HUKUM|MENIMBANG)[^:]*:?\\s*([^A-Z\\n]{100,800})',\n",
        "            r'(?:Menimbang|Mempertimbangkan)[^:]*([^A-Z\\n]{100,600})',\n",
        "            r'(?:DASAR\\s+HUKUM)[^:]*:?\\s*([^A-Z\\n]{100,600})'\n",
        "        ]\n",
        "\n",
        "        for pattern in argumen_patterns:\n",
        "            match = re.search(pattern, clean_text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                argument = match.group(1).strip()\n",
        "                argument = re.sub(r'\\s+', ' ', argument)\n",
        "                content['argumen_hukum'] = argument[:500]\n",
        "                break\n",
        "\n",
        "        # Extract decision\n",
        "        putusan_patterns = [\n",
        "            r'(?:MENGADILI|MEMUTUSKAN|PUTUSAN)[^:]*:?\\s*([^A-Z\\n]{50,400})',\n",
        "            r'(?:MENYATAKAN|MENGHUKUM)[^:]*:?\\s*([^A-Z\\n]{50,400})',\n",
        "        ]\n",
        "\n",
        "        for pattern in putusan_patterns:\n",
        "            match = re.search(pattern, clean_text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                decision = match.group(1).strip()\n",
        "                decision = re.sub(r'\\s+', ' ', decision)\n",
        "                content['putusan'] = decision[:300]\n",
        "                break\n",
        "\n",
        "        # Extract evidence\n",
        "        bukti_patterns = [\n",
        "            r'(?:BARANG\\s+BUKTI|ALAT\\s+BUKTI)[^:]*:?\\s*([^A-Z\\n]{50,300})',\n",
        "            r'(?:BUKTI[^:]*):?\\s*([^A-Z\\n]{50,200})',\n",
        "        ]\n",
        "\n",
        "        for pattern in bukti_patterns:\n",
        "            match = re.search(pattern, clean_text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                evidence = match.group(1).strip()\n",
        "                evidence = re.sub(r'\\s+', ' ', evidence)\n",
        "                content['barang_bukti'] = evidence[:200]\n",
        "                break\n",
        "\n",
        "        return content\n",
        "\n",
        "    def simple_tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Simple tokenization without NLTK\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            List of tokens\n",
        "        \"\"\"\n",
        "        # Convert to lowercase and split\n",
        "        text = text.lower()\n",
        "        # Remove punctuation and split\n",
        "        tokens = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
        "        # Remove stopwords\n",
        "        clean_tokens = [token for token in tokens\n",
        "                       if token not in self.stopwords_id and len(token) > 2]\n",
        "        return clean_tokens\n",
        "\n",
        "    def feature_engineering(self, text: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Perform simple feature engineering\n",
        "\n",
        "        Args:\n",
        "            text: Full text of court decision\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing engineered features\n",
        "        \"\"\"\n",
        "        features = {}\n",
        "\n",
        "        try:\n",
        "            # Basic text statistics\n",
        "            features['text_length'] = len(text)\n",
        "            words = text.split()\n",
        "            features['word_count'] = len(words)\n",
        "\n",
        "            # Simple tokenization\n",
        "            clean_tokens = self.simple_tokenize(text)\n",
        "            features['clean_word_count'] = len(clean_tokens)\n",
        "\n",
        "            # Bag of words (top 15 most common words)\n",
        "            if clean_tokens:\n",
        "                word_freq = Counter(clean_tokens)\n",
        "                top_words = dict(word_freq.most_common(15))\n",
        "                features['top_words'] = json.dumps(top_words)\n",
        "            else:\n",
        "                features['top_words'] = '{}'\n",
        "\n",
        "            # Legal term frequency\n",
        "            legal_terms = ['pasal', 'hukum', 'putusan', 'dakwaan', 'bukti',\n",
        "                          'saksi', 'terdakwa', 'jaksa', 'hakim', 'pengadilan',\n",
        "                          'perdata', 'pidana', 'penggugat', 'tergugat']\n",
        "\n",
        "            legal_term_count = {}\n",
        "            text_lower = text.lower()\n",
        "            for term in legal_terms:\n",
        "                count = text_lower.count(term)\n",
        "                legal_term_count[term] = count\n",
        "\n",
        "            features['legal_terms'] = json.dumps(legal_term_count)\n",
        "\n",
        "            # Sentence count (approximation)\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            features['sentence_count'] = len([s for s in sentences if len(s.strip()) > 10])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in feature engineering: {str(e)}\")\n",
        "            # Default values\n",
        "            features.update({\n",
        "                'text_length': len(text),\n",
        "                'word_count': len(text.split()),\n",
        "                'clean_word_count': 0,\n",
        "                'top_words': '{}',\n",
        "                'legal_terms': '{}',\n",
        "                'sentence_count': 0\n",
        "            })\n",
        "\n",
        "        return features\n",
        "\n",
        "    def create_simple_qa_pairs(self, metadata: Dict, content: Dict) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Create simple QA pairs from the case data\n",
        "\n",
        "        Args:\n",
        "            metadata: Case metadata\n",
        "            content: Case content\n",
        "\n",
        "        Returns:\n",
        "            List of QA pairs\n",
        "        \"\"\"\n",
        "        qa_pairs = []\n",
        "\n",
        "        # QA pairs for metadata\n",
        "        if metadata.get('no_perkara'):\n",
        "            qa_pairs.append({\n",
        "                'question': 'Apa nomor perkara ini?',\n",
        "                'answer': metadata['no_perkara']\n",
        "            })\n",
        "\n",
        "        if metadata.get('tanggal'):\n",
        "            qa_pairs.append({\n",
        "                'question': 'Kapan tanggal putusan ini?',\n",
        "                'answer': metadata['tanggal']\n",
        "            })\n",
        "\n",
        "        if metadata.get('jenis_perkara'):\n",
        "            qa_pairs.append({\n",
        "                'question': 'Apa jenis perkara ini?',\n",
        "                'answer': metadata['jenis_perkara']\n",
        "            })\n",
        "\n",
        "        if metadata.get('pasal'):\n",
        "            qa_pairs.append({\n",
        "                'question': 'Pasal apa yang terlibat dalam perkara ini?',\n",
        "                'answer': metadata['pasal']\n",
        "            })\n",
        "\n",
        "        if metadata.get('penggugat'):\n",
        "            qa_pairs.append({\n",
        "                'question': 'Siapa penggugat dalam perkara ini?',\n",
        "                'answer': metadata['penggugat']\n",
        "            })\n",
        "\n",
        "        # QA pairs for content\n",
        "        if content.get('ringkasan_fakta'):\n",
        "            qa_pairs.append({\n",
        "                'question': 'Apa ringkasan fakta perkara ini?',\n",
        "                'answer': content['ringkasan_fakta'][:200]\n",
        "            })\n",
        "\n",
        "        if content.get('putusan'):\n",
        "            qa_pairs.append({\n",
        "                'question': 'Apa putusan dalam perkara ini?',\n",
        "                'answer': content['putusan'][:200]\n",
        "            })\n",
        "\n",
        "        return qa_pairs\n",
        "\n",
        "    def process_single_case(self, pdf_file_path: str) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Process a single case file\n",
        "\n",
        "        Args:\n",
        "            pdf_file_path: Path to PDF file\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing all processed case data or None if failed\n",
        "        \"\"\"\n",
        "        filename = os.path.basename(pdf_file_path)\n",
        "\n",
        "        try:\n",
        "            # Extract text\n",
        "            text = self.extract_text_from_pdf(pdf_file_path)\n",
        "            if not text or len(text) < 100:\n",
        "                print(f\"  ⚠️  Warning: No text or insufficient text extracted from {filename}\")\n",
        "                return None\n",
        "\n",
        "            # Extract metadata\n",
        "            metadata = self.extract_metadata(text, filename)\n",
        "\n",
        "            # Extract key content\n",
        "            content = self.extract_key_content(text)\n",
        "\n",
        "            # Feature engineering\n",
        "            features = self.feature_engineering(text)\n",
        "\n",
        "            # Create QA pairs\n",
        "            qa_pairs = self.create_simple_qa_pairs(metadata, content)\n",
        "\n",
        "            # Combine all data\n",
        "            case_data = {\n",
        "                **metadata,\n",
        "                **content,\n",
        "                **features,\n",
        "                'text_full': text[:1500],  # Limit full text to first 1500 chars\n",
        "                'qa_pairs': json.dumps(qa_pairs, ensure_ascii=False),\n",
        "                'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            }\n",
        "\n",
        "            return case_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error processing {filename}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def process_all_cases(self) -> None:\n",
        "        \"\"\"\n",
        "        Process all PDF files in the directory\n",
        "        \"\"\"\n",
        "        print(f\"🔍 Checking PDF directory: {self.pdf_path}\")\n",
        "\n",
        "        if not os.path.exists(self.pdf_path):\n",
        "            print(f\"❌ PDF directory not found: {self.pdf_path}\")\n",
        "            return\n",
        "\n",
        "        # Get all PDF files\n",
        "        all_files = os.listdir(self.pdf_path)\n",
        "        pdf_files = [f for f in all_files if f.lower().endswith('.pdf')]\n",
        "\n",
        "        if not pdf_files:\n",
        "            print(f\"❌ No PDF files found in {self.pdf_path}\")\n",
        "            print(f\"   Files found: {all_files[:10]}\")  # Show first 10 files\n",
        "            return\n",
        "\n",
        "        print(f\"📁 Found {len(pdf_files)} PDF files to process\")\n",
        "        print(\"─\" * 50)\n",
        "\n",
        "        successful_cases = 0\n",
        "        failed_cases = 0\n",
        "\n",
        "        for i, filename in enumerate(pdf_files, 1):\n",
        "            pdf_file_path = os.path.join(self.pdf_path, filename)\n",
        "\n",
        "            print(f\"📄 Processing {i}/{len(pdf_files)}: {filename}\")\n",
        "\n",
        "            case_data = self.process_single_case(pdf_file_path)\n",
        "            if case_data:\n",
        "                self.cases_data.append(case_data)\n",
        "                successful_cases += 1\n",
        "                print(f\"  ✅ Success\")\n",
        "            else:\n",
        "                failed_cases += 1\n",
        "                print(f\"  ❌ Failed\")\n",
        "\n",
        "            # Progress update every 10 files\n",
        "            if i % 10 == 0:\n",
        "                print(f\"📊 Progress: {i}/{len(pdf_files)} files processed\")\n",
        "\n",
        "        print(\"─\" * 50)\n",
        "        print(f\"🎯 Processing Complete!\")\n",
        "        print(f\"   ✅ Successful: {successful_cases}\")\n",
        "        print(f\"   ❌ Failed: {failed_cases}\")\n",
        "        print(f\"   📊 Total: {len(pdf_files)}\")\n",
        "\n",
        "    def save_results(self) -> None:\n",
        "        \"\"\"\n",
        "        Save processed cases to both CSV and JSON formats\n",
        "        \"\"\"\n",
        "        if not self.cases_data:\n",
        "            print(\"❌ No data to save\")\n",
        "            return\n",
        "\n",
        "        print(\"💾 Saving results...\")\n",
        "\n",
        "        # Save to CSV\n",
        "        try:\n",
        "            df = pd.DataFrame(self.cases_data)\n",
        "            csv_file_path = os.path.join(self.csv_path, \"cases.csv\")\n",
        "            df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
        "            print(f\"✅ CSV saved: {csv_file_path}\")\n",
        "            print(f\"   📊 Shape: {df.shape}\")\n",
        "            print(f\"   📝 Columns: {len(df.columns)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving CSV: {str(e)}\")\n",
        "\n",
        "        # Save to JSON\n",
        "        try:\n",
        "            json_file_path = os.path.join(self.csv_path, \"cases.json\")\n",
        "            with open(json_file_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.cases_data, f, ensure_ascii=False, indent=2)\n",
        "            print(f\"✅ JSON saved: {json_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error saving JSON: {str(e)}\")\n",
        "\n",
        "    def print_sample_data(self, n: int = 2) -> None:\n",
        "        \"\"\"\n",
        "        Print sample processed data\n",
        "\n",
        "        Args:\n",
        "            n: Number of samples to show\n",
        "        \"\"\"\n",
        "        if not self.cases_data:\n",
        "            print(\"❌ No data available\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n📋 Sample Data (showing first {min(n, len(self.cases_data))} cases):\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for i, case in enumerate(self.cases_data[:n]):\n",
        "            print(f\"\\n🔍 Case {i+1}: {case.get('filename', 'Unknown')}\")\n",
        "            print(f\"   📄 Case ID: {case.get('case_id', 'N/A')}\")\n",
        "            print(f\"   📋 No Perkara: {case.get('no_perkara', 'N/A')}\")\n",
        "            print(f\"   📅 Tanggal: {case.get('tanggal', 'N/A')}\")\n",
        "            print(f\"   ⚖️  Jenis: {case.get('jenis_perkara', 'N/A')}\")\n",
        "            print(f\"   📜 Pasal: {case.get('pasal', 'N/A')[:100]}...\")\n",
        "            print(f\"   👥 Pihak: {case.get('pihak', 'N/A')[:100]}...\")\n",
        "            print(f\"   📊 Words: {case.get('word_count', 0)}\")\n",
        "\n",
        "            if case.get('ringkasan_fakta'):\n",
        "                print(f\"   📝 Fakta: {case['ringkasan_fakta'][:150]}...\")\n",
        "\n",
        "    def get_summary_statistics(self) -> None:\n",
        "        \"\"\"\n",
        "        Print comprehensive summary statistics\n",
        "        \"\"\"\n",
        "        if not self.cases_data:\n",
        "            print(\"❌ No data processed yet\")\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(self.cases_data)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"📊 CASE REPRESENTATION SUMMARY REPORT\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Basic statistics\n",
        "        print(f\"📁 Total cases processed: {len(df)}\")\n",
        "        print(f\"📄 Average text length: {df['text_length'].mean():.0f} characters\")\n",
        "        print(f\"📝 Average word count: {df['word_count'].mean():.0f} words\")\n",
        "        print(f\"🔤 Average clean word count: {df['clean_word_count'].mean():.0f} words\")\n",
        "\n",
        "        # Data completeness\n",
        "        print(f\"\\n📋 Data Completeness:\")\n",
        "        print(f\"   📋 Cases with case numbers: {df['no_perkara'].notna().sum()} ({df['no_perkara'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "        print(f\"   📅 Cases with dates: {df['tanggal'].notna().sum()} ({df['tanggal'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "        print(f\"   ⚖️  Cases with case types: {df['jenis_perkara'].notna().sum()} ({df['jenis_perkara'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "        print(f\"   📜 Cases with articles: {df['pasal'].notna().sum()} ({df['pasal'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "        print(f\"   👥 Cases with parties: {df['pihak'].notna().sum()} ({df['pihak'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "        print(f\"   📝 Cases with facts: {df['ringkasan_fakta'].notna().sum()} ({df['ringkasan_fakta'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "        print(f\"   ⚖️  Cases with decisions: {df['putusan'].notna().sum()} ({df['putusan'].notna().sum()/len(df)*100:.1f}%)\")\n",
        "\n",
        "        # Case type distribution\n",
        "        if df['jenis_perkara'].notna().sum() > 0:\n",
        "            print(f\"\\n⚖️  Case Type Distribution:\")\n",
        "            case_types = df['jenis_perkara'].value_counts()\n",
        "            for case_type, count in case_types.head(10).items():\n",
        "                print(f\"   📁 {case_type}: {count} cases ({count/len(df)*100:.1f}%)\")\n",
        "\n",
        "        # Text statistics\n",
        "        print(f\"\\n📊 Text Statistics:\")\n",
        "        print(f\"   📄 Min text length: {df['text_length'].min():.0f} characters\")\n",
        "        print(f\"   📄 Max text length: {df['text_length'].max():.0f} characters\")\n",
        "        print(f\"   📝 Min word count: {df['word_count'].min():.0f} words\")\n",
        "        print(f\"   📝 Max word count: {df['word_count'].max():.0f} words\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Helper functions for easy usage\n",
        "def create_case_system(pdf_directory: str, output_directory: str = \"./output\") -> SimpleCaseRepresentationSystem:\n",
        "    \"\"\"\n",
        "    Create and initialize a SimpleCaseRepresentationSystem\n",
        "\n",
        "    Args:\n",
        "        pdf_directory: Path to directory containing PDF files\n",
        "        output_directory: Path to save processed results\n",
        "\n",
        "    Returns:\n",
        "        Initialized SimpleCaseRepresentationSystem instance\n",
        "    \"\"\"\n",
        "    return SimpleCaseRepresentationSystem(pdf_directory, output_directory)\n",
        "\n",
        "def process_legal_documents(pdf_directory: str, output_directory: str = \"./output\",\n",
        "                          show_samples: bool = True, sample_count: int = 2) -> SimpleCaseRepresentationSystem:\n",
        "    \"\"\"\n",
        "    Complete pipeline to process legal documents\n",
        "\n",
        "    Args:\n",
        "        pdf_directory: Path to directory containing PDF files\n",
        "        output_directory: Path to save processed results\n",
        "        show_samples: Whether to show sample processed data\n",
        "        sample_count: Number of samples to show\n",
        "\n",
        "    Returns:\n",
        "        SimpleCaseRepresentationSystem instance with processed data\n",
        "    \"\"\"\n",
        "    # Initialize system\n",
        "    system = create_case_system(pdf_directory, output_directory)\n",
        "\n",
        "    # Process all cases\n",
        "    system.process_all_cases()\n",
        "\n",
        "    # Save results\n",
        "    system.save_results()\n",
        "\n",
        "    # Show statistics\n",
        "    system.get_summary_statistics()\n",
        "\n",
        "    # Show samples if requested\n",
        "    if show_samples and system.cases_data:\n",
        "        system.print_sample_data(sample_count)\n",
        "\n",
        "    return system\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example 1: Basic usage\n",
        "    pdf_path = \"/content/drive/MyDrive/PENALARAN KOMPUTER FIX/PDF\"  # Change this to your PDF directory\n",
        "    output_path = \"/content/drive/MyDrive/PENALARAN KOMPUTER FIX\"      # Change this to your desired output directory\n",
        "\n",
        "    print(\"🚀 Starting Case Representation System\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Process all documents\n",
        "    system = process_legal_documents(pdf_path, output_path, show_samples=True, sample_count=3)\n",
        "\n",
        "    print(\"\\n🎉 Processing completed!\")\n",
        "    print(\"Files saved:\")\n",
        "    print(f\"  📄 CSV: {output_path}/cases.csv\")\n",
        "    print(f\"  📄 JSON: {output_path}/cases.json\")\n",
        "\n",
        "    # Example 2: Advanced usage with custom processing\n",
        "    \"\"\"\n",
        "    # Initialize system manually\n",
        "    system = SimpleCaseRepresentationSystem(pdf_path, output_path)\n",
        "\n",
        "    # Process specific file\n",
        "    single_case = system.process_single_case(\"/path/to/specific/file.pdf\")\n",
        "    if single_case:\n",
        "        print(\"Single case processed successfully!\")\n",
        "        print(f\"Case ID: {single_case.get('case_id', 'N/A')}\")\n",
        "\n",
        "    # Process all files\n",
        "    system.process_all_cases()\n",
        "\n",
        "    # Get detailed statistics\n",
        "    system.get_summary_statistics()\n",
        "\n",
        "    # Save results\n",
        "    system.save_results()\n",
        "\n",
        "    # Show sample data\n",
        "    system.print_sample_data(n=5)\n",
        "    \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBVWc7xCU5HH",
        "outputId": "91e69643-2e07-4d88-a3ad-a312d4b2bd36"
      },
      "id": "gBVWc7xCU5HH",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Case Representation System\n",
            "==================================================\n",
            "Initialized Case Representation System\n",
            "PDF Path: /content/drive/MyDrive/PENALARAN KOMPUTER FIX/PDF\n",
            "CSV Path: /content/drive/MyDrive/PENALARAN KOMPUTER FIX\n",
            "🔍 Checking PDF directory: /content/drive/MyDrive/PENALARAN KOMPUTER FIX/PDF\n",
            "📁 Found 117 PDF files to process\n",
            "──────────────────────────────────────────────────\n",
            "📄 Processing 1/117: zaf04bfe9b9d478480e1313134323135.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 2/117: zaf04be947f6488ea6c5303930393335.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 3/117: zaf04bf54310144c9b17313033353231.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 4/117: zaf04bf612a8aa8482bc313034313039.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 5/117: zaf04bf96390c604a4ee313130343533.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 6/117: zaf04bf90032b946ae2f313130323037.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 7/117: zaf04bfec6df42b29505313134333237.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 8/117: zaf04bf353acb24e813d313032313330.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 9/117: zaf04b3a846431a0874a313231383334.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 10/117: zaf04b371e495498a0d2313135343135.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 10/117 files processed\n",
            "📄 Processing 11/117: zaf04c017a89dc30943a313230323438.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 12/117: zaf04bee13e3c16699e4303934333535.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 13/117: zaf04bfc374844e88b53313132353037.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 14/117: zaf04b2fc1646468994f313130313332.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 15/117: zaf04bfb1793560cbec6313131373035.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 16/117: zaf04be554ca15eeadd2303834313139.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 17/117: zaf04bf9aee7d868bfec313130373030.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 18/117: zaf04b2e3cc45a5cb4de313035303430.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 19/117: zaf0370dfb24dede910a323030393233.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 20/117: zaf04bfe64e49f6c80c5313134303433.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 20/117 files processed\n",
            "📄 Processing 21/117: zaf04bf72b52cf968621313034393030.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 22/117: zaf04c0356beb1d48da7313231363037.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 23/117: zaf04beeffb6d4ee8e05303934333231.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 24/117: zaf04c02048773fc8840313230363339.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 25/117: zaf04c0355a04cfea531313231363035.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 26/117: zaf04bf1a2219e78aa9d313030393232.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 27/117: zaf04bed03bcd238b54f303933363139.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 28/117: zaf04c03bed430b49b20313231393031 (1).pdf\n",
            "  ✅ Success\n",
            "📄 Processing 29/117: zaf04bff8870c838b184313134383532.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 30/117: zaf04bf2fd3c3448b20a313031393035.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 30/117 files processed\n",
            "📄 Processing 31/117: 8b5f0132761fceceea0c4f4985eb59fd.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 32/117: 9c29eb9420ee28869fe8530b2f0e21b4.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 33/117: zaf04bf02a5cc93690fc303935383532.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 34/117: zaf04bdedf16d824991d303735353034.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 35/117: zaf04bef03a3baee8c5b303935303337.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 36/117: zaf04bf4be4711ca93fa313033313338.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 37/117: zaf04bf602664e2e818c313034303432.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 38/117: zaf04bfaaf33c0b0a383313131343130.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 39/117: zaf04c03bed430b49b20313231393031.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 40/117: zaf04bf47b3821c6bf3c313032393435.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 40/117 files processed\n",
            "📄 Processing 41/117: zaf04bff550f3b288d18313134373236.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 42/117: zaf04c01721b3c56b51a313230323334.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 43/117: zaf04bf892607732a880313035393032.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 44/117: zaf04b45d2fd1588a8d9313333393331.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 45/117: zaf04b3abeac93b6b6df313232303132.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 46/117: zaf04bd53f834c10812e303634363131.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 47/117: zaf04bf32b3e18e8a5c3313032303232.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 48/117: 11f04c054b040f0e99b5313233303036 (1).pdf\n",
            "  ✅ Success\n",
            "📄 Processing 49/117: zaf04b213a7e8176a0cf303931373333.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 50/117: zaf04bf4305d49a680d9313032373430.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 50/117 files processed\n",
            "📄 Processing 51/117: zaf04bfadb9cd55690ac313131353234.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 52/117: 11f04c054d3fae22aa9f313233303130 (2).pdf\n",
            "  ✅ Success\n",
            "📄 Processing 53/117: 11f04c054b040f0e99b5313233303036.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 54/117: 11f04c054c87129094ba313233303039 (1).pdf\n",
            "  ✅ Success\n",
            "📄 Processing 55/117: zaf04b3b297d83949f16313232333131.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 56/117: zaf04b4bc238bb988e56313432323030.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 57/117: zaf04bdbd6b45e48a145303733333232.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 58/117: zaf04b414c7b94169387313330373037.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 59/117: zaf04be0380ca49e8760303830343433.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 60/117: zaf04b4bd1bde2b49032313432323236.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 60/117 files processed\n",
            "📄 Processing 61/117: zaf04b46d471b3f08509313334363433.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 62/117: zaf04b69a9a1219c9638313735363033.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 63/117: zaf04b69ac875d5e92dc313735363038.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 64/117: zaf04b543a16fbcc9490313532323337.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 65/117: zaf04bfb9ae094c09332313132303435.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 66/117: 11f04c054d3fae22aa9f313233303130 (1).pdf\n",
            "  ✅ Success\n",
            "📄 Processing 67/117: zaf04bdbe52ca444a1a1303733333436.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 68/117: zaf04b4cebdf2f1ca052313433303139.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 69/117: zaf04b57bc5670ec8097313534373434.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 70/117: zaf04b552bfcd1dc968b313532393233.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 70/117 files processed\n",
            "📄 Processing 71/117: zaf04b59687124c0be4e313535393432.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 72/117: zaf04b4c931d06ecb876313432373530.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 73/117: zaf04b871e5eaf3c91f5323132363535.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 74/117: zaf04b48fbc208689ba4313430323038.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 75/117: zaf04bdb038c189e9da5303732373237.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 76/117: zaf04bebd1e6b4aa9b08303932373435.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 77/117: zaf04be0617b6b948f34303830353532.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 78/117: zaf04b3faa4db3faa9ff313235353236.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 79/117: 11f04c054d3fae22aa9f313233303130.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 80/117: zaf04b3d95e29504acce313234303332.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 80/117 files processed\n",
            "📄 Processing 81/117: zaf04b25602f9244b265303934373134.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 82/117: 11f04c054c87129094ba313233303039.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 83/117: zaf04b38279a9510a539313230313430.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 84/117: zaf04bd542982edeb510303634363136.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 85/117: zaf04b3d8f2dec54a5e9313234303231.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 86/117: zaf04bd544ad4e34b2bd303634363230.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 87/117: zaf04be9cfa3c608af8e303931333233.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 88/117: zaf04b492302653aa8bc313430333134.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 89/117: zaf04b4cc2b97854a827313432393130.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 90/117: zaf04b86f147fa6cbc9a323132353339.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 90/117 files processed\n",
            "📄 Processing 91/117: zaf04b4dae7b0cee9659313433353436.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 92/117: zaf04b35be5ffe3eb1b1313134343234.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 93/117: zaf04bf30985e7dab2b0313031393235.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 94/117: zaf04be0333f9e628f73303830343335.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 95/117: zaf04bebccf0e5c49ee1303932373337.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 96/117: zaf04b5e7c5819bca3ee313633363033.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 97/117: zaf04b457b09d8fc8e5c313333373033.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 98/117: zaf04b56c9d89340b4ef313534303537.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 99/117: zaf04b5e835731d0b213313633363135.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 100/117: zaf04bdbd28bf358a3a2303733333135.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 100/117 files processed\n",
            "📄 Processing 101/117: zaf04bd992070f868983303731373037.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 102/117: zaf04be14f75d528b45e303831323332.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 103/117: zaf04b5ec9fa72dcacee313633383133.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 104/117: zaf04b58269e87bea6cf313535303432.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 105/117: zaf04b415933dfcea422313330373239.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 106/117: zaf04b499604a7f09fe6313430363237.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 107/117: zaf04b56ea77099283b8313534313532.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 108/117: zaf04b3d8a3b4df49d62313234303133.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 109/117: zaf04c05450ae168ad29313232393536.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 110/117: zaf04b4fa85509f88325313434393534.pdf\n",
            "  ✅ Success\n",
            "📊 Progress: 110/117 files processed\n",
            "📄 Processing 111/117: zaf04b4893e4fac0ace7313335393133.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 112/117: zaf04b4c0c1335e0c0b3313432343034.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 113/117: zaf04bffb2583384b69d313135303032.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 114/117: zaf04b4613c183f6b220313334313139.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 115/117: zaf04c0481e1c684874f313232343239.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 116/117: zaf04b86ee60f60abfbb323132353334.pdf\n",
            "  ✅ Success\n",
            "📄 Processing 117/117: zaf04b45424b7372a05c313333353238.pdf\n",
            "  ✅ Success\n",
            "──────────────────────────────────────────────────\n",
            "🎯 Processing Complete!\n",
            "   ✅ Successful: 117\n",
            "   ❌ Failed: 0\n",
            "   📊 Total: 117\n",
            "💾 Saving results...\n",
            "✅ CSV saved: /content/drive/MyDrive/PENALARAN KOMPUTER FIX/cases.csv\n",
            "   📊 Shape: (117, 22)\n",
            "   📝 Columns: 22\n",
            "✅ JSON saved: /content/drive/MyDrive/PENALARAN KOMPUTER FIX/cases.json\n",
            "\n",
            "============================================================\n",
            "📊 CASE REPRESENTATION SUMMARY REPORT\n",
            "============================================================\n",
            "📁 Total cases processed: 117\n",
            "📄 Average text length: 53876 characters\n",
            "📝 Average word count: 10405 words\n",
            "🔤 Average clean word count: 4891 words\n",
            "\n",
            "📋 Data Completeness:\n",
            "   📋 Cases with case numbers: 117 (100.0%)\n",
            "   📅 Cases with dates: 117 (100.0%)\n",
            "   ⚖️  Cases with case types: 117 (100.0%)\n",
            "   📜 Cases with articles: 117 (100.0%)\n",
            "   👥 Cases with parties: 117 (100.0%)\n",
            "   📝 Cases with facts: 117 (100.0%)\n",
            "   ⚖️  Cases with decisions: 117 (100.0%)\n",
            "\n",
            "⚖️  Case Type Distribution:\n",
            "   📁 perkara pidana: 82 cases (70.1%)\n",
            "   📁 : 12 cases (10.3%)\n",
            "   📁 Pid.Sus: 10 cases (8.5%)\n",
            "   📁 PID.SUS: 5 cases (4.3%)\n",
            "   📁 Pdt.G: 2 cases (1.7%)\n",
            "   📁 perkara perdata: 2 cases (1.7%)\n",
            "   📁 perkara Pidana: 1 cases (0.9%)\n",
            "   📁 PID.SUS : 1 cases (0.9%)\n",
            "   📁 Pid.I.A.1.3 R a i P U T U S A N s M Nomor 278: 1 cases (0.9%)\n",
            "   📁 Pid.I.A.3 R a i P U T U S A N s M Nomor 759: 1 cases (0.9%)\n",
            "\n",
            "📊 Text Statistics:\n",
            "   📄 Min text length: 8344 characters\n",
            "   📄 Max text length: 213285 characters\n",
            "   📝 Min word count: 2359 words\n",
            "   📝 Max word count: 40467 words\n",
            "\n",
            "============================================================\n",
            "\n",
            "📋 Sample Data (showing first 3 cases):\n",
            "================================================================================\n",
            "\n",
            "🔍 Case 1: zaf04bfe9b9d478480e1313134323135.pdf\n",
            "   📄 Case ID: zaf04bfe9b9d478480e1313134323135\n",
            "   📋 No Perkara: 432/PID.SUS/2025\n",
            "   📅 Tanggal: 13 Agustus 2004\n",
            "   ⚖️  Jenis: perkara pidana\n",
            "   📜 Pasal: 114 a R a i s M g e Halaman, 114 ayat, 111 a i s ayat, 111 d g ayat, 114 n Aayat...\n",
            "   👥 Pihak: Terdakwa: I hNama lengkap : MUHAMMAD BAYU AkJI PAMUNGKAS BIN a (ALM) SLAMET | Terdakwa: Muhammad Bay...\n",
            "   📊 Words: 8673\n",
            "\n",
            "🔍 Case 2: zaf04be947f6488ea6c5303930393335.pdf\n",
            "   📄 Case ID: zaf04be947f6488ea6c5303930393335\n",
            "   📋 No Perkara: 119/PID.SUS/2025\n",
            "   📅 Tanggal: 1 Juli 2003\n",
            "   ⚖️  Jenis: perkara pidana\n",
            "   📜 Pasal: 114 Ayat, 132 ayat, 132 ayat e, 114 ayat, 112 ayat...\n",
            "   👥 Pihak: Terdakwa: I Terdakwa I: h 1. Nama : RUSLAN | Terdakwa: II: g e 1. Nama n: JUPRI | Terdakwa: ditangka...\n",
            "   📊 Words: 11232\n",
            "\n",
            "🔍 Case 3: zaf04bf54310144c9b17313033353231.pdf\n",
            "   📄 Case ID: zaf04bf54310144c9b17313033353231\n",
            "   📋 No Perkara: \n",
            "   📅 Tanggal: \n",
            "   ⚖️  Jenis: \n",
            "   📜 Pasal: ...\n",
            "   👥 Pihak: ...\n",
            "   📊 Words: 4381\n",
            "\n",
            "🎉 Processing completed!\n",
            "Files saved:\n",
            "  📄 CSV: /content/drive/MyDrive/PENALARAN KOMPUTER FIX/cases.csv\n",
            "  📄 JSON: /content/drive/MyDrive/PENALARAN KOMPUTER FIX/cases.json\n"
          ]
        }
      ]
    }
  ]
}